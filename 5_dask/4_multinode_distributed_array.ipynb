{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Array multinode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Architecture for Multinode Execution\n\nIn a typical Dask distributed setup for High-Performance Computing (HPC) environments, the architecture involves three main components:\n\n1.  **Jupyter Notebook (Client)**: This is where your Python code runs, acting as the client that submits tasks to the Dask scheduler. It's usually running on a login node or a dedicated JupyterLab server.\n2.  **Dask Scheduler**: The central component that coordinates all Dask workers. It receives tasks from the client, assigns them to workers, and tracks their progress. In this multinode setup, the scheduler is typically launched on the same node as the Jupyter Notebook client.\n3.  **Dask Workers**: These are the computational engines that execute the actual tasks. In a multinode setup, workers are launched on separate compute nodes, often as part of a SLURM job or similar resource allocation. They connect to the scheduler and perform computations on their assigned data.\n\nThis separation allows for efficient resource utilization and scaling across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AA = da.random.normal(0,1,size=(200000,200000), chunks=(1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "address = socket.gethostbyname(socket.gethostname())\n",
    "print(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [